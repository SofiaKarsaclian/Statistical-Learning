---
title: "Stat Learning Project"
author: "Sof√≠a Karsaclian, Elias Heppner, Andri Rutschmann"
format: html
editor: visual
---

## Statistical Learning Course Project

-   Sof√≠a Karsaclian (01/1246546)
-   Elias Heppner (01/1244414)
-   Andri Rutschmann (01/1246603)

```{r Setup}
#| label: setup
#| message: false

library(tidyverse)
library(tidymodels)
library(xgboost)
library(doParallel)
library(caret)
library(psych)
library(glmnet)
library(pROC)

seed = 3 # because we're three pros working on this project!
target_metric = 'recall' # measure of model performance

# Used to speed up XGBoost with parallel processing
all_cores = detectCores(logical = TRUE)
cl = makePSOCKcluster(all_cores)
registerDoParallel(cl)

# Preprocessing
data = read.csv('BreastCancer.csv') # load data (there is an NA col nameed X)
sum(is.na(data$X)) # so col X is only NA lets remove it

data = data %>%
  select(-X, -id) # take out id
any(is.na(data)) # there are no NA rows in the data set
```

Okay... Let's go!

```{r Exploration}
describe(data)#get an overview 

prop.table(table(data$diagnosis))
barplot(table(data$diagnosis)) # simple bar plot showing beningn/malignant dist

# Filter variables containing the word "mean"
mean_vars <- names(data)[grep("mean", names(data))]

# Plot histograms for variables containing the word "mean"
par(mfrow = c(ceiling(length(mean_vars) / 2), 2))  # Set up multiple plots per page
for (var in mean_vars){
  hist(data[[var]], main = var, xlab = var, breaks = 20, mar = c(6,6,4,4))
}
#-> not normally distributed, all right-skewed

se_vars <- names(data)[grep("se", names(data))]

# Set up multiple scatterplots
par(mfrow = c(ceiling(length(se_vars) / 2), 2))  # Set up multiple plots per page

# Create scatterplots for variables containing the word "se" against the target variable
for (var in se_vars) {
  hist(data[[var]], main = var, xlab = var, breaks = 20, mar = c(4,4,2,2))
}
```

```{r Features Eli}
alpha <- 0.05

target <- data %>%
                 mutate(diagnosis = ifelse(diagnosis == "M", 1, 0)) %>%
                  select(diagnosis)
target <- target$diagnosis

#Check correlations between target variable and others in a comprehensible fashion
association_list <- lapply(data[-c(1:2)], function(var) {
  cor_val <- cor(var, target, method = "pearson")  # Point Biserial Correlation as numeric and binary variable
  return(cor_val)
})

#Print results
names(association_list) <- names(data)[-c(1:2)] 
print(association_list)



# Perform ANOVA for each variable against the target variable
result_list <- lapply(data[-c(1:2)], function(var) {
  aov_result <- aov(target ~ var, data = data)
  p_value <- summary(aov_result)[[1]]$"Pr(>F)"[1]
  significant <- ifelse(p_value < alpha, "Yes", "No")
  return(data.frame(p_value = p_value, significant = significant))
})

# Print ANOVA results for each variable
names(result_list) <- names(data)[-c(1:2)]  # Assign variable names to ANOVA results
print(result_list)


#Findings:

#Correlation with Target Variable Below 0.5:
# 
# texture_mean
#  0.4151853
# smoothness_mean
#  0.3585
# symmetry_mean
#  0.3304986
# fractal_dimension_mean
#  -0.0128376
# texture_se
#  -0.008303333
# smoothness_se
#  -0.06701601
# concavity_se
#  0.2537298
# symmetry_se
#  -0.006521756
# fractal_dimension_se
#  0.07797242
# texture_worst
#  0.4569028
# smoothness_worst
#  0.4214649
# symmetry_worst
#  0.4162943
# fractal_dimension_worst
#  0.3238722

#Below 0.2 for fractal_dimension_mean, texture_se, smoothness_se, symmetry_se, fractal_dimension_se
#-> That's also where the anova test yields insignificant results. Dropping these features likely will not decrease model performance.
```

```{r Feature Selection}
# Feature selection? Not really needed
dim(data)

df_corr <- cor(data %>% select(-diagnosis))
corrplot::corrplot(df_corr, order = "hclust", tl.cex = 1, addrect = 8)

data2 <- data %>% select(-findCorrelation(df_corr, cutoff = 0.9))
detach("package:caret", unload = TRUE) # causes problems with metrics
```

# Model Comparison

We are going to compare two different models for this project: eXtreme Gradient Boosting and Lasso Regression.

Due to this choice we do not need to perform feature selection. XGBoost being a decision tree based model does no

## eXtreme Gradient Boosting (XGBoost)

The first model we want to investigate is the 'Kaggle-famous' XGBoost model. Known for its impressive performance on many Kaggle challenges.

```{r Gradient Boosting}
#| warnings: false

# For model evaluation it is easiest to set the positive (malignant) as fist level
data$diagnosis = factor(data$diagnosis, levels = c('M', 'B'))
data2 = data %>% 
  select(-fractal_dimension_mean, -texture_se, -smoothness_se, -symmetry_se, -fractal_dimension_se)

# Creating train/test splits
set.seed(seed)
training_split = initial_split(data, prop = 0.8, strata = diagnosis)

train = training(training_split)
test = testing(training_split) # we don't really need this since we use last_fit()

# Create tuning recipe
tune_rec = recipe(diagnosis ~ ., data = train)

# Specify k-fold CV (10-fold)
set.seed(seed)
folds = vfold_cv(train, v = 10)

# XGBoost model specification
# xg_spec = boost_tree(trees = tune()) %>%
#   set_engine("xgboost") %>%
#   set_mode("classification")

xg_spec = boost_tree(trees = tune(),
                     tree_depth = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

# xg_spec = boost_tree(trees = tune(),
#                      min_n = tune(),
#                      tree_depth = tune(),
#                      learn_rate = tune(),
#                      loss_reduction = tune()) %>%
#   set_engine("xgboost") %>%
#   set_mode("classification")

# XGBoost work flow
xg_wf = workflow() %>% 
  add_recipe(tune_rec) %>% 
  add_model(xg_spec)

# Lambda grid for model tuning
# lambda_grid = grid_regular(
#   trees(range = c(1, 100)),
#   levels = c(trees = 100)
# )

lambda_grid = grid_regular(
  trees(range = c(1, 60)),
  tree_depth(range = c(1, 30)),
  levels = c(trees = 10,
             tree_depth  = 10)
)

# lambda_grid = grid_regular(
#   trees(range = c(1, 60)),
#   min_n(range = c(1, 10)),
#   tree_depth(range = c(1, 30)),
#   learn_rate(range = c(0, 1)),
#   loss_reduction(range = c(0, 1)),
#   levels = c(trees = 10,
#              min_n = 10,
#              tree_depth  = 10,
#              learn_rate = 10,
#              loss_reduction = 10)
# )

# Model training
metrics = metric_set(f_meas, precision, recall, roc_auc) # specify metrics

set.seed(seed)
xg_rs = tune_grid( # specify individual components with tune_grid
  xg_wf, # workflow to be used
  folds, # different folds
  grid = lambda_grid, # lambda grid for tuning
  control = control_resamples(save_pred = TRUE), # save all metrics (not just default)
  metrics = metrics # specify metrics
)

# autoplot(xg_rs)

xg_rs %>% 
  show_best(target_metric)

# collect_metrics(xg_rs, summarize = TRUE) # get model metrics
best_model = select_best(xg_rs, metric = target_metric) # select best model according to target metric
print(best_model)

# Use best model for finalization
xg_wf_final = finalize_workflow(
  xg_wf, best_model
  )

# Get model performance on test split
xg_final = last_fit(
  xg_wf_final, training_split, metrics = metrics
  )

collect_metrics(xg_final)

# Save... a bit unnecessary since training only takes a few seconds
# saveRDS(xg_final, "xg_trees_depth_n_rate_reduction.RDS")

# collect_predictions(xg_final) %>% 
#   select(id, .row, diagnosis, .pred_class)
```

```{r Load XGBoost Model(s)}
xg_final_1 = readRDS("xg_trees.RDS")
xg_final_2 = readRDS("xg_trees_depth.RDS")
xg_final_3 = readRDS("xg_trees_depth_n_rate_reduction.RDS")

collect_metrics(xg_final_1)
collect_metrics(xg_final_2)
collect_metrics(xg_final_3)
```


```{r Lasso Regression}
# specify model
lasso_spec <- logistic_reg(penalty = 0.01, mixture = 1) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

# specify workflow
lasso_wf <- workflow() %>%
  add_recipe(tune_rec) %>%
  add_model(lasso_spec)

# 10 fold CV
set.seed(seed)
diag_folds <- vfold_cv(train, v = 10)

set.seed(seed)
lasso_rs <- fit_resamples(
  lasso_wf,
  diag_folds,
  control = control_resamples(save_pred = TRUE),
  metrics = metrics
)

# metrics from cv
lasso_rs_metrics <- collect_metrics(lasso_rs)
lasso_rs_predictions <- collect_predictions(lasso_rs)

lasso_rs_metrics
# roc curve - pretty neat
lasso_rs_predictions %>%
  group_by(id) %>%
  roc_curve(truth = diagnosis, .pred_M) %>%
  autoplot() 
```

Through the penalty parameter in Lasso classification we can do feature selection. We initialized the lasso classificator with a penalty of 0.01 and fitted this initial model into a 10-fold cross-validation of the training data. Even with this initial model, the ROC-AUC plot shows that the performance is excellent (ùúá= 0.998). Recall, however, is slightly lower, and this is a very important metric in cancer detection, so we are going to try to improve it through hyperparameter tuning.

```{r Tuning Lassos penalty parameter}
# Tuning for choosing best penalty - although pretty good already
# model tuning
tune_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

# create a regular grid of values to try
# grid_regular chooses sensible values to try for regularization penalty
lambda_grid <- grid_regular(penalty(), levels = 100)
# lambda_grid

# workflow for tuning
tune_wf <- workflow() %>%
  add_recipe(tune_rec) %>%
  add_model(tune_spec)

set.seed(seed)
tune_rs <- tune_grid(
  tune_wf,
  diag_folds,
  grid = lambda_grid,
  control = control_resamples(save_pred = TRUE),
  metrics = metrics
)

autoplot(tune_rs) 

tune_rs %>% # show the best models and their respective penalties
  show_best(target_metric) 

chosen_l1 <- tune_rs %>%
  select_best(metric = target_metric)

print(chosen_l1)
```

We tune the model's $l_1$ penalty by creating a grid of 100 sensible values to try with `grid_regular`. We learn that the value of $l_1$ that yields the best recall (ùúá= 0.993) is 0.00059, and that this value also yields high roc_auc.

```{r}
final_lasso <- finalize_workflow(tune_wf, chosen_l1)

fitted_lasso <- fit(final_lasso, train)

# fitted_lasso %>%
#   extract_fit_parsnip() %>%
#   tidy() %>%
#   arrange(estimate)

# Evaluate on test data 
lasso_final <- last_fit(final_lasso, split = training_split, metrics = metrics)
collect_metrics(lasso_final, summarize = TRUE)


# Plot ROC curve
predictions <- lasso_final %>%
  collect_predictions()

roc_data <- roc(predictions$diagnosis, predictions$.pred_M)
plot(roc_data, main = "ROC Curve", col = "blue", lwd = 2)
```

Finally, we fit the Lasso with the tuned penalty to the train data, and evaluate the performance on the test data. The model performs really well, with a high capability of distinguishing classes (ROC AUC = 98.12%), and a high recall (95.29%) which means that the model is effective at capturing most positive cases, minimizing false negatives.

```{r SVM}
library(e1071)

svm_model <- svm(diagnosis ~ ., data = train, kernel = "radial")

predictions <- predict(svm_model, newdata = test)
library(caret)

# Assuming 'predictions' are the predicted class labels and 'true_labels' are the true class labels
confusion_matrix <- confusionMatrix(predictions, test$diagnosis)

# Calculate precision
precision <- confusion_matrix$byClass["Precision"]

# Calculate recall (Sensitivity)
recall <- confusion_matrix$byClass["Sensitivity"]

# Calculate F1 score
f1_score <- confusion_matrix$byClass["F1"]

# Calculate AUC-ROC
auc_roc <- confusion_matrix$byClass["ROC"]

# Print or use the metrics as needed
print(precision)
print(recall)
print(f1_score)
print(auc_roc)
```
