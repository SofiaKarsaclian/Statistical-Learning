---
title: "Stat Learning Project"
author: "Sofía, Elias, Andri"
format: html
editor: visual
---

## Statistical Learning Course Project

-   Sofía Karsaclian (01/1246546)
-   Elias Heppner ()
-   Andri Rutschmann (01/1246603)

```{r Setup}
#| label: setup
#| message: false

library(tidyverse)
library(tidymodels)
library(xgboost)
library(doParallel)
library(caret)
#install.packages('glmnet', dependencies=TRUE, type="binary")
library(glmnet)

seed = 3 # because we're three pros working on this project!
target_metric = 'recall' # measure of model performance

# Used to speed up XGBoost with parallel processing
all_cores = detectCores(logical = TRUE)
cl = makePSOCKcluster(all_cores)
registerDoParallel(cl)

# Preprocessing
data = read.csv('BreastCancer.csv') # load data (there is an NA col nameed X)
sum(is.na(data$X)) # so col X is only NA lets remove it
data = data %>% 
  select(-X)
any(is.na(data)) # there are no NA rows in the data set
```

Okay... Let's go!

```{r Exploration}
prop.table(table(data$diagnosis))
barplot(table(data$diagnosis)) # simple bar plot showing beningn/malignant dist
```

```{r}

# Feature selection? Not really needed
dim(data)

df_corr <- cor(data %>% select(-id, -diagnosis))
corrplot::corrplot(df_corr, order = "hclust", tl.cex = 1, addrect = 8)

data2 <- data %>% select(-findCorrelation(df_corr, cutoff = 0.9))
detach("package:caret", unload = TRUE)
```

```{r Gradient Boosting}
# For model evaluation it is easiest to set the positive (malignant) as fist level
data$diagnosis = factor(data$diagnosis, levels = c('M', 'B'))
data = bind_cols(data %>% select(diagnosis), data2)

# Creating train/test splits
set.seed(seed)
training_split = initial_split(data, prop = 0.8, strata = diagnosis)

train = training(training_split)
test = testing(training_split) # we don't really need this since we use last_fit()

# Create tuning recipe
tune_rec = recipe(diagnosis ~ ., data = train)

# Specify k-fold CV (10-fold)
set.seed(seed)
folds = vfold_cv(train, v = 10)

# XGBoost model specification
xg_spec = boost_tree(trees = tune()) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

# XGBoost work flow
xg_wf = workflow() %>% 
  add_recipe(tune_rec) %>% 
  add_model(xg_spec)

# Lambda grid for model tuning
lambda_grid = grid_regular(
  trees(range = c(1, 1000)), # set tree range
  levels = c(trees = 1000) # and levels (how many steps in tree range)
)

# Model training
metrics = metric_set(f_meas, precision, recall, roc_auc) # specify metrics

start = Sys.time() # Get some duration info
paste('Start:', start)

set.seed(seed)
xg_rs = tune_grid( # specify individual components with tune_grid
  xg_wf, # workflow to be used
  folds, # different folds
  grid = lambda_grid, # lambda grid for tuning
  control = control_resamples(save_pred = TRUE), # save all metrics (not just default)
  metrics = metrics # specify metrics
)

print(Sys.time() - start) # How long did the training take?

collect_metrics(xg_rs, summarize = TRUE) # get model metrics
best_model = select_by_pct_loss(xg_rs, metric = target_metric, trees) # 'best' simple model (according to metric) - select_best() gives best model (more complex)
print(best_model)

# Use best model for finalization
xg_wf_final = finalize_workflow(
  xg_wf, best_model
  )

# Get model performance on test split
xg_final = last_fit(
  xg_wf_final, training_split, metrics = metrics
  )

collect_metrics(xg_final)

# Save... a bit unnecessary since training only takes a few seconds
# saveRDS(xg_final, "model_xgboost_feature_selected.RDS")

collect_predictions(xg_final) %>% 
  select(-.config)
```

```{r Lasso Regression}

# specify model
lasso_spec <- logistic_reg(penalty = 0.01, mixture = 1) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

# specify workflow
lasso_wf <- workflow() %>%
  add_recipe(tune_rec) %>%
  add_model(lasso_spec)

# 10 fold CV
set.seed(seed)
diag_folds <- vfold_cv(train)

set.seed(seed)
lasso_rs <- fit_resamples(
  lasso_wf,
  diag_folds,
  control = control_resamples(save_pred = TRUE)
)

# metrics from cv
lasso_rs_metrics <- collect_metrics(lasso_rs)
lasso_rs_predictions <- collect_predictions(lasso_rs)

# roc curve - pretty neat
lasso_rs_predictions %>%
  group_by(id) %>%
  roc_curve(truth = diagnosis, .pred_M) %>%
  autoplot() 

# Tuning for choosing best penalty - although pretty good already
# model tuning
tune_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

# create a regular grid of values to try
# grid_refuls chooses sensible values to try for regularization penalty
lambda_grid <- grid_regular(penalty(), levels = 30)
lambda_grid

# workflow for tuning
tune_wf <- workflow() %>%
  add_recipe(tune_rec) %>%
  add_model(tune_spec)

set.seed(seed)
tune_rs <- tune_grid(
  tune_wf,
  diag_folds,
  grid = lambda_grid,
  control = control_resamples(save_pred = TRUE),
  metrics = metrics
)

autoplot(tune_rs)

tune_rs %>%
  show_best(target_metric) # best ROC value is 99.37

# penalty for best ROC is 0.042
chosen_auc <- tune_rs %>%
  select_by_one_std_err(metric = target_metric, -penalty)

final_lasso <- finalize_workflow(tune_wf, chosen_auc)

fitted_lasso <- fit(final_lasso, train)

fitted_lasso %>%
  extract_fit_parsnip() %>%
  tidy() %>%
  arrange(estimate)

# Chosen terms: concave points(worst and mean), and worst measures for smoothness, symmetry, radius and texture. Worst measure for concave points its the most important predictor.


# Evaluate on test data 
lasso_final <- last_fit(final_lasso, split = training_split, metrics = metrics)
collect_metrics(lasso_final, summarize = TRUE)

```
